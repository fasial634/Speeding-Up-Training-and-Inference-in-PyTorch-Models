{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Optimization Techniques for Faster Training\n- Multi-process Data Loading by increasing the number of worker in DataLoader.\n- The use of pin_memory=True.\n- Larger batch size.\n- Utilizing all CPU cores by using torch.set_num_threads(num_threads).\n- Just-In-Time (JIT) Compilation by using torch.jit.script.\n- using torch.compile(model)","metadata":{}},{"cell_type":"markdown","source":"# Optimization Techniques for Faster Inference\n\n- Dynamic quantization to reduce model weights from float32 to int8.\n- JIT-Compiled to optimize.\n- using torch.inference_mode(), it is faster than torch.no_grad() for inference by skipping gradient and version tracking.\n- ONNX Export","metadata":{}},{"cell_type":"code","source":"!pip install onnxruntime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T11:17:02.377150Z","iopub.execute_input":"2025-04-21T11:17:02.377609Z","iopub.status.idle":"2025-04-21T11:17:10.412729Z","shell.execute_reply.started":"2025-04-21T11:17:02.377567Z","shell.execute_reply":"2025-04-21T11:17:10.411338Z"}},"outputs":[{"name":"stdout","text":"Collecting onnxruntime\n  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nCollecting coloredlogs (from onnxruntime)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\nRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (3.20.3)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (2.4.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->onnxruntime) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->onnxruntime) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.6->onnxruntime) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.6->onnxruntime) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.6->onnxruntime) (2024.2.0)\nDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.21.1\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import torch\nimport os\nimport psutil\n\nnum_threads = os.cpu_count()\nprint(f\"available cpu threads in this machine {num_threads}\")\ntorch.set_num_threads(2)\nprint(f\"{torch.get_num_threads()} CPU threads used by PyTorch.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:55:14.046489Z","iopub.execute_input":"2025-04-21T10:55:14.046933Z","iopub.status.idle":"2025-04-21T10:55:14.052999Z","shell.execute_reply.started":"2025-04-21T10:55:14.046910Z","shell.execute_reply":"2025-04-21T10:55:14.051976Z"}},"outputs":[{"name":"stdout","text":"available cpu threads in this machine 4\n2 CPU threads used by PyTorch.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"num_workers = os.cpu_count()\nprint(f\"available CPU cores (number of threads): {num_workers}\")\n\nprint(f\"number of CPU cores (physical cores): {psutil.cpu_count(logical=False)}\")  # This gives physical cores\nprint(f\"number of CPU threads: {psutil.cpu_count(logical=True)}\")   # This gives logical(number of threads) cores (same as os.cpu_count())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:55:16.696853Z","iopub.execute_input":"2025-04-21T10:55:16.697150Z","iopub.status.idle":"2025-04-21T10:55:16.703767Z","shell.execute_reply.started":"2025-04-21T10:55:16.697131Z","shell.execute_reply":"2025-04-21T10:55:16.702850Z"}},"outputs":[{"name":"stdout","text":"available CPU cores (number of threads): 4\nnumber of CPU cores (physical cores): 2\nnumber of CPU threads: 4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch \nfrom torch import nn \nfrom torch.utils.data import DataLoader\nimport pandas as pd \nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:55:38.237851Z","iopub.execute_input":"2025-04-21T10:55:38.238167Z","iopub.status.idle":"2025-04-21T10:55:39.397857Z","shell.execute_reply.started":"2025-04-21T10:55:38.238143Z","shell.execute_reply":"2025-04-21T10:55:39.397093Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"<h2 style=\"background:#00373E; color: #E3EEFC; border-radius: 4px; padding: 8px 32px;\"><a class=\"anchor\"  id=\"2-2. data-preperation\">1. Data Preparation</a></h2>","metadata":{}},{"cell_type":"code","source":"# load data\ntrain = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\", dtype = np.float32)\n\n# data spliting\nfeatures = train.loc[:, train.columns != \"label\"]\nnp_targets = train.label.values\n\n# normalize features\nnp_features = features.values/255","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:55:45.087913Z","iopub.execute_input":"2025-04-21T10:55:45.088414Z","iopub.status.idle":"2025-04-21T10:55:48.819147Z","shell.execute_reply.started":"2025-04-21T10:55:45.088386Z","shell.execute_reply":"2025-04-21T10:55:48.818124Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:55:48.820613Z","iopub.execute_input":"2025-04-21T10:55:48.820964Z","iopub.status.idle":"2025-04-21T10:55:48.858490Z","shell.execute_reply.started":"2025-04-21T10:55:48.820936Z","shell.execute_reply":"2025-04-21T10:55:48.857425Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0    1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n1    0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n2    1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n3    4.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n4    0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0     0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n1     0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n2     0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n3     0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n4     0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0       0.0       0.0       0.0       0.0  \n1       0.0       0.0       0.0       0.0  \n2       0.0       0.0       0.0       0.0  \n3       0.0       0.0       0.0       0.0  \n4       0.0       0.0       0.0       0.0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 785 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Data visualization\nimages = [120, 121, 122]\nfor image in images:\n    plt.imshow(np_features[image].reshape(28,28), cmap='gray')\n    plt.axis(\"off\")\n    plt.title(str(np_targets[image]))\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:57:46.767841Z","iopub.execute_input":"2025-04-21T10:57:46.768173Z","iopub.status.idle":"2025-04-21T10:57:47.034195Z","shell.execute_reply.started":"2025-04-21T10:57:46.768151Z","shell.execute_reply":"2025-04-21T10:57:47.033360Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# data split for train and testing \nx_train, x_test, y_train, y_test = train_test_split(np_features, np_targets, test_size=0.2, random_state=42)\n\n# conversion from numpy and tensor\nx_train = torch.from_numpy(x_train).type(torch.float32)\nx_test = torch.from_numpy(x_test).type(torch.float32)\ny_train = torch.from_numpy(y_train).type(torch.LongTensor)\ny_test = torch.from_numpy(y_test).type(torch.LongTensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:58:01.202958Z","iopub.execute_input":"2025-04-21T10:58:01.203246Z","iopub.status.idle":"2025-04-21T10:58:01.626771Z","shell.execute_reply.started":"2025-04-21T10:58:01.203228Z","shell.execute_reply":"2025-04-21T10:58:01.625945Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Creating PyTorch train and test datasets\ntrain = torch.utils.data.TensorDataset(x_train, y_train)\ntest = torch.utils.data.TensorDataset(x_test, y_test)\n\n# define batch size for training and testing \n\n# to optimize performance for training:\n# - num_workers\n# - pin_memory=True\n\nnum_workers = 2\n\nbatch_size = 100\ntrain_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\ntest_loader = DataLoader(test, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n\n# train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n# test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:58:11.488448Z","iopub.execute_input":"2025-04-21T10:58:11.488801Z","iopub.status.idle":"2025-04-21T10:58:11.497091Z","shell.execute_reply.started":"2025-04-21T10:58:11.488777Z","shell.execute_reply":"2025-04-21T10:58:11.496103Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"<h2 style=\"background:#00373E; color: #E3EEFC; border-radius: 4px; padding: 8px 32px;\"><a class=\"anchor\"  id=\"2-2. data-preperation\">2. Building ANN Model</a></h2>","metadata":{}},{"cell_type":"code","source":"class ANNModel(nn.Module): \n    \n    ### Define Network Architecture ###\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        \n        super(ANNModel, self).__init__()\n        \n        self.layer1 = nn.Linear(input_dim, hidden_dim)\n        self.act1 = nn.ReLU() # Activation function\n        \n        self.layer2 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.act2 = nn.ReLU()\n        \n        self.layer3 = nn.Linear(hidden_dim // 2, hidden_dim // 4)\n        self.act3 = nn.ReLU() \n        \n        self.layer4 = nn.Linear(hidden_dim // 4, output_dim)\n        \n    ### Define Forward Pass ###\n    def forward(self, x): \n        \n       \n        out = self.layer1(x)\n        out = self.act1(out)\n\n        out = self.layer2(out)\n        out = self.act2(out)\n        \n        out = self.layer3(out)\n        out = self.act3(out)\n    \n        out = self.layer4(out)\n        \n        \n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T11:00:19.887240Z","iopub.execute_input":"2025-04-21T11:00:19.887601Z","iopub.status.idle":"2025-04-21T11:00:19.895291Z","shell.execute_reply.started":"2025-04-21T11:00:19.887577Z","shell.execute_reply":"2025-04-21T11:00:19.894191Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# set the parameters\ninput_dim = 28*28 # the size of the image\nhidden_dim = 160 # open for experiment but 150 as a start\noutput_dim = 10 # number of classes (from 0 to 9)\n\nbatch_size = 100\n# it requires 10000 batches to \nn_iterations = 2000\nnum_epochs = n_iterations / (len(x_train) / batch_size)\nnum_epochs = int(num_epochs)\nprint(f\"number of epochs: {num_epochs}\")\n\n# # Create the model\n# model = ANNModel(input_dim, hidden_dim, output_dim)\n\n\n# ### Set loss, optimizer, and Learning rate ###\n\n# # loss function -> CrossEnropy (check pytorch docs)\n# error = nn.CrossEntropyLoss() \n\n# # Learning rate \n# learning_rate = 0.02 #(just as a start)\n\n# # optimizer -> Stochastic gradient decent (SGD)\n# optim = torch.optim.Adam(model.parameters(),\n#                        lr= learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T11:03:44.256004Z","iopub.execute_input":"2025-04-21T11:03:44.256437Z","iopub.status.idle":"2025-04-21T11:03:44.263267Z","shell.execute_reply.started":"2025-04-21T11:03:44.256411Z","shell.execute_reply":"2025-04-21T11:03:44.262093Z"}},"outputs":[{"name":"stdout","text":"number of epochs: 5\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"<h2 style=\"background:#00373E; color: #E3EEFC; border-radius: 4px; padding: 8px 32px;\"><a class=\"anchor\"  id=\"2-2. data-preperation\">3. Model Training</a></h2>","metadata":{}},{"cell_type":"code","source":"### Model training loop ###\n\nimport torch.profiler as profiler\n\n# Prepare for tracking\nmodel_loss = []\nmodel_accuracy = []\n\n# Set device explicitly to CPU\ndevice = torch.device(\"cpu\")\n\n# --- Integrate the PyTorch Profiler ---\nwith profiler.profile(\n    activities=[\n        profiler.ProfilerActivity.CPU,\n        profiler.ProfilerActivity.CUDA if torch.cuda.is_available() else None,\n    ],\n    record_shapes=True,\n    profile_memory=True,\n    with_stack=True) as prof:\n\n\n    # Create the model\n    model = ANNModel(input_dim, hidden_dim, output_dim).to(device)\n\n    # JIT compiled\n    # model = torch.jit.script(model)\n    # using compile\n    # model = torch.compile(model, mode=\"reduce-overhead\")  \n    \n    error = nn.CrossEntropyLoss() \n    learning_rate = 0.02 \n    optim = torch.optim.Adam(model.parameters(), lr= learning_rate)\n\n    # Start training\n    for epoch in range(num_epochs): \n        model.train()\n\n        running_loss = 0.0\n        total_train = 0\n        correct_train = 0\n\n        for i, (images, labels) in enumerate(train_loader): \n            inputs = images.view(-1, 28*28)\n            \n            optim.zero_grad() \n            \n            outputs = model(inputs)\n            \n            loss = error(outputs, labels)\n            loss.backward() \n            optim.step()\n            \n            # Accumulate loss\n            running_loss += loss.item()\n\n            predicted = torch.max(outputs.data, 1)[1]\n            total_train += labels.size(0)\n            correct_train += (predicted == labels).sum().item()\n\n        # Evaluate on test set after each epoch\n        model.eval()\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for images, labels in test_loader: \n                test = images.view(-1, 28*28)\n                outputs = model(test) \n                predicted = torch.max(outputs.data, 1)[1]\n                total += labels.size(0) \n                correct += (predicted == labels).sum().item()\n\n        epoch_loss = running_loss / len(train_loader)\n        epoch_accuracy = 100 * correct / float(total)\n\n        # Store and print per epoch\n        model_loss.append(epoch_loss)\n        model_accuracy.append(epoch_accuracy)\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} | Accuracy: {epoch_accuracy:.2f}%\")\n        \n        prof.step()\n\n# Print the profiler output\n\nprint(\"\\n\", \"CPU's Performance Report\")\nprint(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T11:06:37.062190Z","iopub.execute_input":"2025-04-21T11:06:37.062828Z","iopub.status.idle":"2025-04-21T11:09:22.437075Z","shell.execute_reply.started":"2025-04-21T11:06:37.062796Z","shell.execute_reply":"2025-04-21T11:09:22.436038Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/5] Loss: 0.3463 | Accuracy: 93.51%\nEpoch [2/5] Loss: 0.1970 | Accuracy: 94.69%\nEpoch [3/5] Loss: 0.1732 | Accuracy: 94.71%\nEpoch [4/5] Loss: 0.1610 | Accuracy: 93.50%\nEpoch [5/5] Loss: 0.1585 | Accuracy: 95.55%\n\n CPU's Performance Report\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nenumerate(DataLoader)#_MultiProcessingDataLoaderIter...        33.58%        8.553s        33.88%        8.631s       4.090ms     629.65 Mb     628.05 Mb          2110  \n                               Optimizer.step#Adam.step         9.49%        2.418s        30.68%        7.816s       4.653ms       1.08 Mb      -1.78 Gb          1680  \n    autograd::engine::evaluate_function: AddmmBackward0         0.78%     198.018ms        13.33%        3.396s     505.422us     904.46 Mb    -185.85 Mb          6720  \n                                         AddmmBackward0         0.71%     181.768ms        11.68%        2.976s     442.838us       1.06 Gb           0 b          6720  \n                                           aten::linear         0.35%      88.823ms        10.02%        2.553s     303.889us     232.32 Mb           0 b          8400  \n                                               aten::mm         9.92%        2.527s         9.96%        2.536s     215.677us       1.06 Gb       1.06 Gb         11760  \n                                            aten::addmm         6.93%        1.766s         8.81%        2.245s     267.252us     232.32 Mb     232.32 Mb          8400  \n                                             aten::sqrt         6.54%        1.666s         6.54%        1.666s     123.932us     910.87 Mb     910.87 Mb         13440  \n                                              aten::div         4.07%        1.037s         4.86%        1.238s      92.082us     910.87 Mb     910.82 Mb         13440  \n                                               aten::to         0.56%     143.070ms         3.26%     830.855ms      14.436us       1.81 Mb           0 b         57556  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 25.473s\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"<h2 style=\"background:#00373E; color: #E3EEFC; border-radius: 4px; padding: 8px 32px;\"><a class=\"anchor\"  id=\"2-2. data-preperation\">4. Comparative Analysis of Techniques for Faster Inference</a></h2>","metadata":{}},{"cell_type":"code","source":"import time\nimport os\nimport torch\nimport torch.onnx\nimport torch.nn as nn\nimport numpy as np\nimport onnxruntime as ort\nfrom onnxruntime.quantization import quantize_dynamic, QuantType\n\nresults = []\n\n# ------------------------------\n# Helper: File Size\n# ------------------------------\ndef get_file_size(filename):\n    size_mb = os.path.getsize(filename) / (1024 * 1024)\n    print(f\"üì¶ {filename} size: {size_mb:.2f} MB\")\n    return size_mb\n\n# ------------------------------\n# PyTorch Evaluation\n# ------------------------------\ndef evaluate_model(model, name, save_path=None):\n    model.eval()\n    correct = 0\n    total = 0\n    start_time = time.time()\n\n    with torch.inference_mode():\n        for images, labels in test_loader:\n            images = images.view(-1, 28 * 28)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    end_time = time.time()\n    acc = 100 * correct / total\n    dur = end_time - start_time\n    size = get_file_size(save_path) if save_path else 0\n    results.append((name, acc, dur, size))\n    print(f\"‚ö° {name} - Accuracy: {acc:.2f}% | Time: {dur:.2f} sec\")\n\n# ------------------------------\n# ONNX Evaluation\n# ------------------------------\ndef evaluate_onnx_model(path, name):\n    session = ort.InferenceSession(path)\n    correct, total = 0, 0\n    start = time.time()\n\n    for images, labels in test_loader:\n        images = images.view(-1, 28 * 28).numpy().astype(np.float32)\n        outputs = session.run(None, {\"input\": images})[0]\n        predicted = np.argmax(outputs, axis=1)\n        total += labels.size(0)\n        correct += (predicted == labels.numpy()).sum().item()\n\n    end = time.time()\n    acc = 100 * correct / total\n    dur = end - start\n    size = get_file_size(path)\n    results.append((name, acc, dur, size))\n    print(f\"‚ö° {name} - Accuracy: {acc:.2f}% | Time: {dur:.2f} sec\")\n\n\n\n# ------------------------------\n# 1. Save PyTorch Model (Optional)\n# ------------------------------\ntorch.save(model.state_dict(), \"model_fp32.pth\")\nevaluate_model(model, \"FP32\", \"model_fp32.pth\")\n\n# ------------------------------\n# 2. JIT Compile\n# ------------------------------\njit_fp32 = torch.jit.script(model)\njit_fp32.save(\"model_fp32_jit.pt\")\nevaluate_model(jit_fp32, \"JIT FP32\", \"model_fp32_jit.pt\")\n\n# ------------------------------\n# 3. Dynamic Quantization + JIT\n# ------------------------------\nquantized = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\njit_quant = torch.jit.script(quantized)\njit_quant.save(\"model_quant_jit.pt\")\nevaluate_model(jit_quant, \"JIT Quantized INT8\", \"model_quant_jit.pt\")\n\n# ------------------------------\n# 4. Export to ONNX (FP32)\n# ------------------------------\ndummy_input = torch.randn(1, 28 * 28)\ntorch.onnx.export(\n    model, dummy_input, \"model_fp32.onnx\",\n    input_names=[\"input\"], output_names=[\"output\"],\n    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n    opset_version=11\n)\nevaluate_onnx_model(\"model_fp32.onnx\", \"ONNX FP32\")\n\n# ------------------------------\n# 5. ONNX Quantization (INT8)\n# ------------------------------\nquantize_dynamic(\"model_fp32.onnx\", \"model_quant.onnx\", weight_type=QuantType.QInt8)\nevaluate_onnx_model(\"model_quant.onnx\", \"ONNX Quantized INT8\")\n\n# ------------------------------\n# 6. TorchDynamo + TorchInductor (PyTorch 2.x)\n# ------------------------------\ncompiled_model = torch.compile(model, mode=\"default\", backend=\"inductor\")\nevaluate_model(compiled_model, \"Torch.compile FP32\")\n\n# ------------------------------\n# 6. Summary Table\n# ------------------------------\nprint(\"\\nüìä Inference Summary:\\n\")\nprint(\"{:<25} {:>10} {:>15} {:>15}\".format(\"Model\", \"Accuracy\", \"Time (sec)\", \"Size (MB)\"))\nprint(\"-\" * 70)\nfor name, acc, dur, size in results:\n    print(f\"{name:<25} {acc:10.2f} {dur:15.2f} {size:15.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T11:28:09.743244Z","iopub.execute_input":"2025-04-21T11:28:09.743645Z","iopub.status.idle":"2025-04-21T11:28:16.040053Z","shell.execute_reply.started":"2025-04-21T11:28:09.743615Z","shell.execute_reply":"2025-04-21T11:28:16.038758Z"}},"outputs":[{"name":"stdout","text":"üì¶ model_fp32.pth size: 0.55 MB\n‚ö° FP32 - Accuracy: 95.55% | Time: 1.10 sec\nüì¶ model_fp32_jit.pt size: 0.55 MB\n‚ö° JIT FP32 - Accuracy: 95.55% | Time: 1.01 sec\nüì¶ model_quant_jit.pt size: 0.15 MB\n‚ö° JIT Quantized INT8 - Accuracy: 95.60% | Time: 1.02 sec\nüì¶ model_fp32.onnx size: 0.54 MB\n‚ö° ONNX FP32 - Accuracy: 95.55% | Time: 0.98 sec\nüì¶ model_quant.onnx size: 0.14 MB\n‚ö° ONNX Quantized INT8 - Accuracy: 95.57% | Time: 0.96 sec\n‚ö° Torch.compile FP32 - Accuracy: 95.55% | Time: 1.04 sec\n\nüìä Inference Summary:\n\nModel                       Accuracy      Time (sec)       Size (MB)\n----------------------------------------------------------------------\nFP32                           95.55            1.10            0.55\nJIT FP32                       95.55            1.01            0.55\nJIT Quantized INT8             95.60            1.02            0.15\nONNX FP32                      95.55            0.98            0.54\nONNX Quantized INT8            95.57            0.96            0.14\nTorch.compile FP32             95.55            1.04            0.00\n","output_type":"stream"}],"execution_count":33}]}