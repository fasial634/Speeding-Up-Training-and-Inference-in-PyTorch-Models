# üöÄ Speeding Up Training and Inference in PyTorch Models

This repository provides a **practical guide to optimizing the performance of PyTorch models** for both training and inference, particularly in **CPU environments**. It walks through various techniques to speed up execution, reduce model size, and improve deployment readiness ‚Äî all using a simple neural network on MNIST-style datasets.

---

## üîç Techniques Covered

| Optimization Method            | Purpose                                |
|--------------------------------|----------------------------------------|
| ‚úÖ JIT Compilation (`torch.jit`)        | Speeds up inference via graph compilation |
| ‚úÖ Dynamic Quantization                | Reduces model size and improves CPU performance |
| ‚úÖ ONNX Export & Runtime               | Enables hardware-agnostic inference and cross-platform support |
| ‚úÖ ONNX Quantization (INT8)            | Further model compression with minor accuracy loss |
| ‚úÖ `torch.compile()` (PyTorch 2.x)     | Compiler-based backend acceleration |
| ‚úÖ TorchDynamo + TorchInductor         | Just-in-time graph capture and compiler backend |
| üì¶ Model Size, Accuracy & Speed Report | Analyze trade-offs visually with an inference summary |

---
